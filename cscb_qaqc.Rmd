---
title: "cscb_qaqc"
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(substr(inputFile,1,nchar(inputFile)-4),Sys.Date(),'.html')) })

output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: false
    number_sections: true
    highlight: pygments 
    theme: cosmo
    code_folding: hide
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
email = 1

source('r_scripts/load.R') #libraries
source('r_scripts/UPAS_functions.R') #functions to clean, plot, summarize


```



# Cleaning rules

* remove mission with start time set to 1969
* remove missions & events with start time more than 30 days before s6 (randomization) date
* remove events before install in intervention hh, keep all dates in control
* keep only post-randomization events
* remove data on or after day with thermocouple error. Truncate mission time/observation time after this so as not to overinflate non-event time. (Keep data before thermocouple error from dot that starts recording in the future afterward)
* remove short events (< 5 mins on all stoves) 
* *NEW* remove missions that ran for less than one day
* [still working on how to calculate valid observational time given multiple dots in households - probably have to only include the time when all known dots on a particular type of stove (e.g. traditional) were functioning?]
* events are received in UTC and are converted to local time in this cleaning step.


# Data
## Load data
``` {r load-data, cache = TRUE}

  # UPAS data import and check
upasfilepath <- "~/Dropbox/World Bank CSCB Field Folder/Field Data/UPAS Data" 

file_list_upas <- list.files(upasfilepath, pattern='.txt|.TXT', full.names = T,recursive = T) %>% 
  grep("DIAGNOSTIC", ., ignore.case = TRUE, value = TRUE, invert = TRUE)  %>% 
  grep("2021-3-9_34DACON_C_PS1473_C0214", ., ignore.case = TRUE, value = TRUE, invert = TRUE)  
  
file_list_upas = file_list_upas[sapply(file_list_upas, file.size) > 10000]

upas_data = rbindlist(lapply(file_list_upas[1:9],read_upas),fill=TRUE) 


upas_header <- rbindlist(lapply(file_list_upas,read_upas_header)) %>% 
  as.data.frame() %>% 
  dplyr::arrange(UPASlogFilename,StartDateTimeUTC) %>% 
  dplyr::mutate(file_start_date = as.Date(StartDateTimeUTC),
                CartridgeID = toupper(CartridgeID),
                StartDateTimeLocal = with_tz(StartDateTimeUTC, 
                                             tzone="America/Los_Angeles"),
                date_start = date(StartDateTimeLocal)) %>%
  dplyr::select(-UPASfirmware,-LifetimeSampleCount,-LifetimeSampleRuntime,-GPSUTCOffset,ProgrammedStartDelay,-AppLock,-AppVersion)
  # dplyr::filter(date(StartDateTimeLocal) %in% date(field_log$excel_datetime_start) | 
                  # hour(StartDateTimeLocal) == 0)

#Import list of filters assembled in Colorado
# filter_list = readxl::read_xlsx(list.files(path = "~/Dropbox/Coachella Field/Data",
#                                            pattern = 'AspenData', recursive = T, full.names = T),
#                                 skip = 0,sheet = 'Filter List')[,2:5] 
# 
# colnames(filter_list) <-c("FilterID", "FilterType","CartridgeID","DateSent")
# filter_list <- filter_list %>% 
#   dplyr::filter(!is.na(DateSent)) %>% 
#   dplyr::mutate(DateSent = as.Date(DateSent),
#                 FilterID = toupper(FilterID),
#                 CartridgeID = toupper(CartridgeID),
#                 FilterID = case_when(FilterType == "Quartz" ~ paste0(as.character(DateSent),"_",CartridgeID),
#                                      TRUE ~ FilterID))

```


```{r sendemails}


list_of_datasets <- list(
  "UPAS"=upas_header
  # "Deployment Summary"=deployment_check
)
filename = paste0("QA_Reports/CSCB_QA_Report_",  Sys.Date(), ".xlsx")
write.xlsx(list_of_datasets,file = filename)

source('r_scripts/emailfun.R')
emailfun(email,filename)





```